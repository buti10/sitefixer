# app/modules/wp_repair/modules/malware/scan.py
from __future__ import annotations

import re
import math
import hashlib
from stat import S_ISDIR
from typing import Any, Dict, List, Optional, Iterator

from app.modules.wp_repair.modules.sftp.connect import sftp_connect
from app.modules.wp_repair.db_audit import add_finding, flush_findings
from app.modules.wp_repair.modules.malware.yara_loader import yara_match
from app.modules.wp_repair.modules.malware.yara_manager import get_yara


def _sha256(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()


def _entropy(b: bytes) -> float:
    if not b:
        return 0.0
    from collections import Counter

    c = Counter(b)
    n = len(b)
    e = 0.0
    for v in c.values():
        p = v / n
        e -= p * math.log2(p)
    return e


def _line_and_snippet(text: str, offset: Optional[int]) -> tuple[Optional[int], Optional[str]]:
    if not text or offset is None or offset < 0:
        return None, None
    line = text.count("\n", 0, offset) + 1
    start = text.rfind("\n", 0, offset) + 1
    end = text.find("\n", offset)
    if end == -1:
        end = len(text)
    snippet = text[start:end]
    if len(snippet) > 400:
        snippet = snippet[:400] + " â€¦"
    return line, snippet


_REGEXES = [
    ("dangerous_functions", "medium",
     re.compile(r"\b(eval|assert|system|shell_exec|exec|passthru|popen|proc_open)\s*\(", re.I)),
    ("obfuscation", "medium",
     re.compile(r"(base64_decode\s*\(|gzinflate\s*\(|str_rot13\s*\(|preg_replace\s*\(.*/e)", re.I)),
    ("php_backdoor_io", "medium",
     re.compile(r"(php://input|php://filter|data://text/plain|expect://)", re.I)),
]

_USER_INPUT_RE = re.compile(r"\$_(GET|POST|REQUEST|COOKIE|SERVER)\b", re.I)

_DEFAULT_EXTS = (
    ".php", ".phtml", ".php5", ".js",
    ".htaccess", ".txt", ".html", ".css", ".json", ".svg",
    ".ini", ".conf",
)

_SKIP_DIRS = {".git", "node_modules", "vendor", "storage", "cache", ".sitefixer"}


def _detect_heuristic(content: bytes) -> tuple[List[dict], str]:
    try:
        text = content.decode("utf-8", errors="ignore")
    except Exception:
        text = ""

    finds: List[dict] = []
    for kind, sev, rgx in _REGEXES:
        for m in rgx.finditer(text):
            finds.append(
                {
                    "engine": "heuristic",
                    "kind": kind,
                    "severity": sev,
                    "info": f"{kind} match",
                    "offset": m.start(),
                    "match": m.group(0),
                }
            )

    ent = _entropy(content)
    if ent >= 7.8 and len(content) > 300:
        finds.append(
            {
                "engine": "heuristic",
                "kind": "high_entropy",
                "severity": "low",
                "info": f"{ent:.2f}",
                "offset": None,
                "match": None,
            }
        )

    return finds, text


def _score(text: str, finds: List[dict], path: str) -> tuple[str, int]:
    has_obf = any(f["kind"] == "obfuscation" for f in finds)
    has_danger = any(f["kind"] == "dangerous_functions" for f in finds)
    has_entropy = any(f["kind"] == "high_entropy" for f in finds)
    has_input = bool(_USER_INPUT_RE.search(text))

    score = 0
    if has_obf and has_danger and has_input:
        score += 60
    elif has_obf and has_danger:
        score += 45
    elif has_obf and has_input:
        score += 40
    elif has_danger and has_input:
        score += 35
    elif has_obf:
        score += 20
    elif has_danger:
        score += 15

    if has_entropy:
        score += 10

    plow = (path or "").lower()
    if "/uploads/" in plow or "/upload/" in plow or "/cache/" in plow or "/tmp/" in plow:
        score += 10

    if score >= 60:
        return "malicious", score
    if score >= 30:
        return "suspicious", score
    return "clean", score


def _clamp_start_path(wp_root: str, start_path: Optional[str]) -> str:
    if not wp_root:
        raise ValueError("wp_root required")

    boundary = wp_root.rstrip("/")
    root = (start_path or wp_root).rstrip("/")
    if not (root == boundary or root.startswith(boundary + "/")):
        raise ValueError("start_path outside wp_root boundary")
    return root


def _iter_candidates_sftp(
    sftp,
    root: str,
    *,
    max_files: int,
    exts: tuple[str, ...],
    errors: List[dict],
) -> Iterator[str]:
    emitted = 0

    def is_candidate(p: str) -> bool:
        return p.lower().endswith(exts)

    def walk_dir(p: str):
        nonlocal emitted
        if emitted >= max_files:
            return
        try:
            for a in sftp.listdir_attr(p):
                if emitted >= max_files:
                    return
                name = a.filename
                if name in _SKIP_DIRS:
                    continue
                full = (p.rstrip("/") + "/" + name) if p != "/" else ("/" + name)
                if S_ISDIR(a.st_mode):
                    yield from walk_dir(full)
                else:
                    if is_candidate(full):
                        emitted += 1
                        yield full
        except Exception as e:
            errors.append({"path": p, "error": str(e)})

    try:
        st = sftp.stat(root)
        if S_ISDIR(st.st_mode):
            yield from walk_dir(root)
        else:
            if is_candidate(root):
                emitted += 1
                yield root
    except Exception as e:
        errors.append({"path": root, "error": str(e)})


def _log_finding_safe(action_id: str, *, path: str, severity: str, kind: str, detail_json: dict) -> bool:
    try:
        add_finding(
            action_id,
            path=path,
            severity=severity,
            kind=kind,
            detail_json=detail_json,
            commit=False,
        )
        return True
    except Exception:
        return False


def malware_scan_apply(
    host: str,
    port: int,
    username: str,
    password: str,
    *,
    wp_root: str,
    action_id: str,
    max_files: int = 4000,
    max_bytes: int = 2_000_000,
    exts: tuple[str, ...] = _DEFAULT_EXTS,
    start_path: str | None = None,
    yara_enabled: bool = True,
    connect_timeout: int = 15,
    op_timeout: int = 20,
    batch_commit: int = 50,
) -> Dict[str, Any]:
    root = _clamp_start_path(wp_root, start_path)

    yara_rules, yara_info = get_yara(enabled=yara_enabled)

    pending = 0
    BATCH = max(1, int(batch_commit))

    def log_and_maybe_flush(*, path: str, severity: str, kind: str, detail_json: dict):
        nonlocal pending
        if _log_finding_safe(action_id, path=path, severity=severity, kind=kind, detail_json=detail_json):
            pending += 1
            if pending >= BATCH:
                flush_findings(commit=True)
                pending = 0

    # YARA status (1x)
    log_and_maybe_flush(
        path=root,
        severity="low" if (yara_rules is not None and yara_info.enabled and not yara_info.error) else "medium",
        kind="yara_status",
        detail_json={
            "enabled_param": yara_enabled,
            "enabled_effective": bool(yara_rules is not None and yara_info.enabled),
            "error": yara_info.error,
            "ruleset": yara_info.ruleset_name,
            "ruleset_version": yara_info.ruleset_version,
            "yara_version": yara_info.yara_version,
            "rules_dir": yara_info.ruleset_path,
            "rule_files": yara_info.rule_files,
            "compiled_files": yara_info.compiled_files,
            "skipped_files_sample": (yara_info.skipped_files or [])[:20],
        },
    )

    client = sftp_connect(
        host,
        port,
        username,
        password,
        connect_timeout=connect_timeout,
        op_timeout=op_timeout,
    )
    sftp = client.open_sftp()

    errors: List[dict] = []
    counts = {
        "total_candidates": 0,
        "scanned": 0,
        "clean": 0,
        "suspicious": 0,
        "malicious": 0,
        "errors": 0,
        # NEW (compact counters)
        "matched_files": 0,        # 1x per file (if yara hit OR heuristic suspicious/malicious)
        "yara_matches_total": 0,   # sum of rule matches across files
    }

    try:
        any_candidate = False

        for path in _iter_candidates_sftp(sftp, root, max_files=max_files, exts=exts, errors=errors):
            any_candidate = True
            counts["total_candidates"] += 1

            try:
                with sftp.file(path, "rb") as f:
                    content = f.read(max_bytes)

                sha = _sha256(content)

                # --- YARA ---
                yara_hits: List[dict] = []
                if yara_rules is not None:
                    yara_hits = yara_match(yara_rules, data=content, filepath=path) or []
                    counts["yara_matches_total"] += len(yara_hits)

                # --- Heuristic ---
                h_finds, text = _detect_heuristic(content)
                level, score = _score(text, h_finds, path)

                # Compact finding decision:
                is_matched = False
                final_level = "clean"

                if yara_hits:
                    is_matched = True
                    final_level = "malicious"
                else:
                    # only heuristic can match
                    if level in ("suspicious", "malicious") and h_finds:
                        is_matched = True
                        final_level = level
                    else:
                        final_level = "clean"

                if is_matched:
                    counts["matched_files"] += 1

                    sev = "high" if final_level == "malicious" else "medium"

                    # compact yara list
                    yara_compact = []
                    for hit in yara_hits:
                        yara_compact.append(
                            {
                                "rule": hit.get("rule"),
                                "namespace": hit.get("namespace"),
                                "tags": hit.get("tags"),
                                "meta": hit.get("meta"),
                            }
                        )

                    # compact heuristic list with snippets
                    heur_compact = []
                    for fd in (h_finds or []):
                        line, snippet = _line_and_snippet(text, fd.get("offset"))
                        heur_compact.append(
                            {
                                "kind": fd.get("kind"),
                                "info": fd.get("info"),
                                "match": fd.get("match"),
                                "line": line,
                                "snippet": snippet,
                                "severity": fd.get("severity"),
                            }
                        )

                    log_and_maybe_flush(
                        path=path,
                        severity=sev,
                        kind="file_match",
                        detail_json={
                            "sha256": sha,
                            "level": final_level,
                            "score": score,
                            "max_bytes": max_bytes,
                            "engines": {
                                "yara": {
                                    "enabled": bool(yara_rules is not None and yara_info.enabled),
                                    "ruleset": yara_info.ruleset_name,
                                    "ruleset_version": yara_info.ruleset_version,
                                    "yara_version": yara_info.yara_version,
                                    "matches_total_for_file": len(yara_hits),
                                    "matches": yara_compact,
                                },
                                "heuristic": {
                                    "finds": heur_compact,
                                },
                            },
                        },
                    )

                # classification counts
                if final_level == "clean":
                    counts["clean"] += 1
                elif final_level == "malicious":
                    counts["malicious"] += 1
                else:
                    counts["suspicious"] += 1

            except Exception as e:
                counts["errors"] += 1
                log_and_maybe_flush(
                    path=path,
                    severity="low",
                    kind="read_error",
                    detail_json={"engine": "reader", "error": str(e)},
                )

            counts["scanned"] += 1

        if not any_candidate:
            log_and_maybe_flush(
                path=root,
                severity="medium",
                kind="scan_empty",
                detail_json={
                    "reason": "no candidates or access denied",
                    "errors_sample": errors[:20],
                    "exts": list(exts),
                    "skip_dirs": sorted(list(_SKIP_DIRS)),
                },
            )

        if pending:
            flush_findings(commit=True)
            pending = 0

        return {
            "ok": True,
            "scanned": counts["scanned"],
            "total_candidates": counts["total_candidates"],
            "counts": counts,
            "errors": errors[:50],
            "yara": {
                "enabled": bool(yara_rules is not None and yara_info.enabled),
                "ruleset": yara_info.ruleset_name,
                "version": yara_info.yara_version,
                "rules_dir": yara_info.ruleset_path,
                "rule_files": yara_info.rule_files,
                "compiled_files": yara_info.compiled_files,
                "skipped_files": (yara_info.skipped_files or [])[:50],
                "error": yara_info.error,
                "ruleset_version": yara_info.ruleset_version,
            },
        }

    finally:
        try:
            if pending:
                flush_findings(commit=True)
        except Exception:
            pass

        try:
            sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass
